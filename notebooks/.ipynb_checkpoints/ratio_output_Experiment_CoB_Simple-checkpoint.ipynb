{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdgemk5nHdbL",
    "outputId": "793bbcbb-96da-4252-a4be-e75389010cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.keras.backend.set_floatx(\n",
    "#     'float32'\n",
    "# )\n",
    "import numpy as np\n",
    "import seaborn as sns; \n",
    "tfd = tf.contrib.distributions\n",
    "import imageio\n",
    "import glob\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.stats import norm, uniform, cauchy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "slim = tf.contrib.slim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "boCNTMt4BEiY"
   },
   "outputs": [],
   "source": [
    "tol = 1e-35\n",
    "bs = 500\n",
    "K = 3\n",
    "do = 0.8\n",
    "\n",
    "def ratios_critic(x, prob = 1, reuse=False):\n",
    "    with tf.variable_scope('critic') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        x = tf.expand_dims(x,1)\n",
    "#         x = slim.fully_connected(x, 100, activation_fn=tf.nn.softplus)\n",
    "        \n",
    "        h1 = slim.fully_connected(x, 20, activation_fn=tf.nn.softplus)\n",
    "        h1 = tf.nn.dropout(h1,prob)\n",
    "        h1 = slim.fully_connected(h1, 10, activation_fn=tf.nn.softplus)\n",
    "        h1 = tf.nn.dropout(h1,prob)\n",
    "        \n",
    "        h2 = slim.fully_connected(x, 20, activation_fn=tf.nn.softplus)\n",
    "        h2 = tf.nn.dropout(h2,prob)\n",
    "        h2 = slim.fully_connected(h2, 10, activation_fn=tf.nn.softplus)\n",
    "        h2 = tf.nn.dropout(h2,prob)\n",
    "        \n",
    "        log_d = tf.concat([h1,h2],1)\n",
    "        log_d = slim.fully_connected(log_d, 2, activation_fn=None)\n",
    "#         log_d = tf.concat([slim.fully_connected(h1, 1, activation_fn=None),slim.fully_connected(h2, 1, activation_fn=None)],1)\n",
    "    return tf.squeeze(log_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "lt5Ozzo0HrUj"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.random.set_random_seed(40)\n",
    "\n",
    "mu_1 = 0.\n",
    "mu_2 = 2.\n",
    "mu_3 = 3.\n",
    "scale_p=0.1\n",
    "scale_q=0.2\n",
    "scale_m=2.\n",
    "\n",
    "p = tfd.Normal(loc=mu_1, scale=scale_p)\n",
    "q = tfd.Normal(loc=mu_2, scale=scale_q)\n",
    "base = tfp.distributions.Cauchy(loc=mu_3, scale=scale_m)\n",
    "\n",
    "\n",
    "samples = base.sample([bs]) \n",
    "p_samples = p.sample([bs]) \n",
    "q_samples = q.sample([bs])\n",
    "m_samples = samples\n",
    "# m_samples = samples + (p_samples+q_samples)/2.\n",
    "m_samples2 = m_samples# samples[bs:]\n",
    "\n",
    "#Ratio by log of prob evaluated at samples from base\n",
    "log_ratio_p_q = p.log_prob(m_samples) - q.log_prob(m_samples)\n",
    "#KL from CoB\n",
    "kld = tf.reduce_mean(log_ratio_p_q) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QH5eP5NclBEf"
   },
   "outputs": [],
   "source": [
    "# Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "\n",
    "ratio_on_p = ratios_critic(p_samples, do)\n",
    "ratio_on_q = ratios_critic(q_samples, do,reuse=True)\n",
    "ratio_on_m = ratios_critic(m_samples, do,reuse=True)\n",
    "# disc_m_gen = ratios_critic(m_samples2, do,reuse=True)\n",
    "\n",
    "logitP = tf.concat([ratio_on_p-tf.expand_dims(base.log_prob(p_samples),1),tf.expand_dims(base.log_prob(p_samples),1)],1)\n",
    "logitQ = tf.concat([ratio_on_q-tf.expand_dims(base.log_prob(q_samples),1),tf.expand_dims(base.log_prob(q_samples),1)],1)\n",
    "logitM = tf.concat([ratio_on_m-tf.expand_dims(base.log_prob(m_samples),1),tf.expand_dims(base.log_prob(m_samples),1)],1)\n",
    "\n",
    "a = np.tile([1,0,0],bs)\n",
    "b = np.tile([0,1,0],bs)\n",
    "c = np.tile([0,0,1],bs)\n",
    "\n",
    "label_a = tf.reshape(a,[bs,K])\n",
    "label_b = tf.reshape(b,[bs,K])\n",
    "label_c = tf.reshape(c,[bs,K])\n",
    "\n",
    "\n",
    "disc_loss_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logitP, labels=label_a))\n",
    "disc_loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logitQ, labels=label_b))\n",
    "disc_loss_3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logitM, labels=label_c))\n",
    "# disc_loss_4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=disc_m_gen, labels=label_c))\n",
    "\n",
    "dloss = disc_loss_1 + disc_loss_2 + 10*disc_loss_3 #+ 0.5*disc_loss_4 \n",
    "\n",
    "# Define CoB DRE\n",
    "\n",
    "log_r_p_m = ratios_critic(m_samples, reuse=True) \n",
    "log_r_p_from_m_direct = log_r_p_m \n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "c_vars = [var for var in t_vars if 'critic' in var.name]\n",
    "\n",
    "c_optim = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(dloss, var_list=t_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "c8oVgSoXEAv_",
    "outputId": "9a5bbab4-c758-4cbb-a3de-25d9ba2bded6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "loss2 = []\n",
    "pbar = (range(0,15000))\n",
    "for i in pbar:\n",
    "    # Train\n",
    "    feed_dict = {}\n",
    "    l2,_ = sess.run([dloss, c_optim],feed_dict=feed_dict)\n",
    "    loss2.append(l2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(loss2, label='CoB')\n",
    "ax.set_xlabel('Iterations' )\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "08mK99A8IL2n",
    "outputId": "35db5c04-7e38-4291-dcc4-b81d576a6b53"
   },
   "outputs": [],
   "source": [
    "# Sampling\n",
    "kl_ratio_store=[]\n",
    "log_ratio_store=[]\n",
    "log_r_p_from_m_direct_store=[]\n",
    "\n",
    "\n",
    "feed_dict = feed_dict\n",
    "kl_ratio, p_s, q_s, d_s, m_s, lpq, lpq_from_cob_dre_direct= sess.run([kld,\n",
    "                                                                            p_samples, q_samples, samples, m_samples,\n",
    "                                                                            log_ratio_p_q,  log_r_p_from_m_direct],\n",
    "                                                                          feed_dict=feed_dict)\n",
    "kl_ratio_store.append(kl_ratio)\n",
    "log_ratio_store.append(lpq)\n",
    "log_r_p_from_m_direct_store.append(lpq_from_cob_dre_direct)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(p_s, density=True, histtype='stepfilled', alpha=0.8, label='P')\n",
    "ax.hist(q_s, density=True, histtype='stepfilled', alpha=0.8, label='Q')\n",
    "# ax.hist(d_s, density=True, histtype='stepfilled', alpha=0.8, label='M')\n",
    "ax.hist(m_s, density=True, histtype='stepfilled', alpha=0.8, label='M')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.xlim(-5,6)\n",
    "# plt.ylim(-400,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ja9hDBCPlg12",
    "outputId": "ae16376f-694d-43a5-e7d5-f830b068474f"
   },
   "outputs": [],
   "source": [
    "xs = m_s\n",
    "plt.scatter(xs,log_ratio_store[0],label='True from log_prob',alpha=0.9,s=10.)\n",
    "plt.scatter(xs,lpq_from_cob_dre_direct[:,0]-lpq_from_cob_dre_direct[:,1],label='from CoB DRE Direct',alpha=0.9,s=10.)\n",
    "\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Log Ratio\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(-6,5)\n",
    "plt.ylim(-600,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "gHtEC7YB-jah",
    "outputId": "fcebaef2-ab26-4892-9f76-626bb3e04810"
   },
   "outputs": [],
   "source": [
    "rat_p = log_r_p_from_m_direct_store[-1][:,0]- cauchy.logpdf(xs,loc=mu_3,scale=scale_m)\n",
    "rat_q = log_r_p_from_m_direct_store[-1][:,1]- cauchy.logpdf(xs,loc=mu_3,scale=scale_m)\n",
    "\n",
    "d = [np.squeeze(norm.logpdf(x,mu_2,scale_q)) for x in xs]\n",
    "b = [np.squeeze(norm.logpdf(x,mu_1,scale_p)) for x in xs]\n",
    "\n",
    "plt.scatter(xs,b,label='True P',alpha=0.9,s=5.)\n",
    "plt.scatter(xs,rat_p,label='P',alpha=0.9,s=5.)\n",
    "plt.scatter(xs,d,label='True q',alpha=0.9,s=5.)\n",
    "plt.scatter(xs,rat_q,label='q',alpha=0.9,s=5.)\n",
    "\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Log P(x)\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(-4.,4)\n",
    "plt.ylim(-600,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ratio_output_Experiment_CoB_Simple.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
